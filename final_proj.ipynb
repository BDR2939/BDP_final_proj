{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Platform\n",
    "## Assignment 2: MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By:**  \n",
    "\n",
    "Roni Ben Dom, 207576463\n",
    "Yotam Lev, 315870964\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prerequisites**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "!pip install --quiet zipfile36\n",
    "!pip install names\n",
    "!pip install Ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "import threading # you can use easier threading packages\n",
    "import sqlite3\n",
    "\n",
    "# ml\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# visual\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# notebook\n",
    "from IPython.display import display\n",
    "\n",
    "#Additionals\n",
    "import names\n",
    "import glob\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hide Warnings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disable Autoscrolling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set Random Seed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "# Question 1\n",
    "# Initial Steps\n",
    "\n",
    "Write Python code to create 20 different CSV files in this format:  `myCSV[Number].csv`, where each file contains 10 records. \n",
    "\n",
    "The schema is `(‘firstname’,’secondname’,city’)`  \n",
    "\n",
    "Values should be randomly chosen from the lists: \n",
    "- `firstname` : `[John, Dana, Scott, Marc, Steven, Michael, Albert, Johanna]`  \n",
    "- `city` : `[New York, Haifa, München, London, Palo Alto,  Tel Aviv, Kiel, Hamburg]`  \n",
    "- `secondname`: any value  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstname = ['John', 'Dana', 'Scott', 'Marc', 'Steven', 'Michael', 'Albert', 'Johanna']\n",
    "city = ['NewYork', 'Haifa', 'Munchen', 'London', 'PaloAlto',  'TelAviv', 'Kiel', 'Hamburg']\n",
    "secondname = [names.get_last_name() for i in range(10)]\n",
    "\n",
    "for i in range(20):\n",
    "    with open(f'csvs/myCSV[{i}].csv', 'w') as f:\n",
    "        f.write('firstname, secondname, city\\n')\n",
    "        num_lines = np.random.randint(100, 200)\n",
    "        for j in range(num_lines):\n",
    "            first_name = firstname[random.randint(0, len(firstname)-1)]\n",
    "            second_name = secondname[random.randint(0, len(secondname)-1)]\n",
    "            city_name = city[random.randint(0, len(city)-1)]\n",
    "            f.write(f'{first_name}, {second_name}, {city_name}\\n')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use python to Create `mapreducetemp` and `mapreducefinal` folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('mapreducetemp', exist_ok=True)\n",
    "os.makedirs('mapreducefinal', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "# Question 2\n",
    "## MapReduceEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write Python code to create an SQLite database with the following table\n",
    "\n",
    "`TableName: temp_results`   \n",
    "`schema: (key:TEXT,value:TEXT)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created successfully\n"
     ]
    }
   ],
   "source": [
    "db_file = 'HW_2.db'\n",
    "\n",
    "#Connecting to sqlite\n",
    "conn = sqlite3.connect(db_file)\n",
    "\n",
    "#Creating a cursor object using the cursor() method\n",
    "cursor = conn.cursor()\n",
    "\n",
    "#Droping mydata table if already exists.\n",
    "cursor.execute(\"DROP TABLE IF EXISTS temp_results\")\n",
    "\n",
    "#Creating table as per requirement\n",
    "sql ='''CREATE TABLE IF NOT EXISTS temp_results(\n",
    "   key TEXT,\n",
    "   value TEXT\n",
    ")'''\n",
    "cursor.execute(sql)\n",
    "print(\"Table created successfully\")\n",
    "\n",
    "# Commiting changes in the database\n",
    "conn.commit()\n",
    "\n",
    "conn = sqlite3.connect(db_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Create a Python class** `MapReduceEngine` with method `def execute(input_data, map_function, reduce_function)`, such that:\n",
    "    - `input_data`: is an array of elements\n",
    "    - `map_function`: is a pointer to the Python function that returns a list where each entry of the form (key,value) \n",
    "    - `reduce_function`: is pointer to the Python function that returns a list where each entry of the form (key,value)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**Implement** the following functionality in the `execute(...)` function:\n",
    "\n",
    "<br>\n",
    "\n",
    "1. For each key  from the  input_data, start a new Python thread that executes map_function(key) \n",
    "<br><br>\n",
    "2. Each thread will store results of the map_function into mapreducetemp/part-tmp-X.csv where X is a unique number per each thread.\n",
    "<br><br>\n",
    "3. Keep the list of all threads and check whether they are completed.\n",
    "<br><br>\n",
    "4. Once all threads completed, load content of all CSV files into the temp_results table in SQLite.\n",
    "\n",
    "    Remark: Easiest way to loop over all CSV files and load them into Pandas first, then load into SQLite  \n",
    "    `data = pd.read_csv(path to csv)`  \n",
    "    `data.to_sql(‘temp_results’,sql_conn, if_exists=’append’,index=False)`\n",
    "<br><br>\n",
    "\n",
    "5. **Write SQL statement** that generates a sorted list by key of the form `(key, value)` where value is concatenation of ALL values in the value column that match specific key. For example, if table has records\n",
    "<table>\n",
    "    <tbody>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center\">John</td>\n",
    "                <td style=\"text-align:center\">myCSV1.csv</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center\">Dana</td>\n",
    "                <td style=\"text-align:center\">myCSV5.csv</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center\">John</td>\n",
    "                <td style=\"text-align:center\">myCSV7.csv</td>\n",
    "            </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "    Then SQL statement will return `(‘John’,’myCSV1.csv, myCSV7.csv’)`  \n",
    "    Remark: use GROUP_CONCAT and also GROUP BY ORDER BY\n",
    "<br><br><br>\n",
    "6. **Start a new thread** for each value from the generated list in the previous step, to execute `reduce_function(key,value)` \n",
    "<br>    \n",
    "7. Each thread will store results of reduce_function into `mapreducefinal/part-X-final.csv` file  \n",
    "<br>\n",
    "8. Keep list of all threads and check whether they are completed  \n",
    "<br>\n",
    "9. Once all threads completed, print on the screen `MapReduce Completed` otherwise print `MapReduce Failed` \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement all of the class here\n",
    "\n",
    "class MapReduceEngine_smallFiles():\n",
    "    def __init__(self, connection, cursor, block_size = 128):\n",
    "        self.conn = connection\n",
    "        self.cursor = cursor\n",
    "        self.block_size = block_size #Block size in MB\n",
    "    \n",
    "    def auxilary_writing_function(self, function, index, title, *args):\n",
    "        return_val = function(*args)\n",
    "        with open(eval(f'f\"{title}\"'), 'w') as f:\n",
    "            f.write('key,value\\n')\n",
    "            for val in return_val:\n",
    "                f.write(f'{val}\\n')\n",
    "    \n",
    "    def execute(self, input_data, map_function, reduce_function, params):\n",
    "\n",
    "        threads = list()\n",
    "        small_files = []\n",
    "        i = 0\n",
    "        for key in input_data:\n",
    "            file_size_bytes = os.path.getsize(key)\n",
    "            if file_size_bytes >= self.block_size * (2 ** 20):\n",
    "                x = threading.Thread(target=self.auxilary_writing_function, args=(map_function, i, f'mapreducetemp/part-tmp-{i}.csv', key, params[key]))\n",
    "                threads.append(x)\n",
    "                x.start()\n",
    "                i += 1\n",
    "            else:\n",
    "                small_files.append([key, file_size_bytes])\n",
    "                \n",
    "        small_files_df = pd.DataFrame(small_files)\n",
    "        small_files_df.columns = ['path', 'size']\n",
    "        small_files_df.sort_values(by = 'size', ignore_index = True, inplace = True)\n",
    "        \n",
    "        index_smallest = 0\n",
    "        index_largest = small_files_df.shape[0] - 1\n",
    "\n",
    "        sum_files_size = 0\n",
    "        list_indexes_list = []\n",
    "        while index_smallest <= index_largest:\n",
    "            sum_files_size = small_files_df.iloc[index_largest]['size']\n",
    "            index_list = []\n",
    "            while index_smallest <= index_largest and sum_files_size + small_files_df.iloc[index_largest]['size'] < self.block_size * (2 ** 20):\n",
    "                sum_files_size += small_files_df.iloc[index_largest]['size']\n",
    "                index_list.append(index_largest)\n",
    "                index_largest -= 1\n",
    "            \n",
    "            while sum_files_size + small_files_df.iloc[index_smallest]['size'] < self.block_size * (2 ** 20) and index_smallest <= index_largest:\n",
    "                sum_files_size += small_files_df.iloc[index_smallest]['size']\n",
    "                index_list.append(index_smallest)\n",
    "                index_smallest += 1\n",
    "                \n",
    "            list_indexes_list.append(index_list)\n",
    "            \n",
    "        for indexes in list_indexes_list:\n",
    "            df_from_each_file = (pd.read_csv(small_files_df.iloc[ind]['path'], sep=',') for ind in indexes)\n",
    "            df_merged = pd.concat(df_from_each_file, ignore_index=True)\n",
    "            df_merged.to_csv(f\"csvs/merged_{i}.csv\", index = False)\n",
    "            x = threading.Thread(target=self.auxilary_writing_function, args=(map_function, i, f'mapreducetemp/part-tmp-{i}.csv', os.path.abspath(f\"csvs/merged_{i}.csv\"), 1))\n",
    "            threads.append(x)\n",
    "            x.start()\n",
    "            i += 1\n",
    "                \n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "        \n",
    "        for index in range(i):\n",
    "            data = pd.read_csv(f'mapreducetemp/part-tmp-{index}.csv')\n",
    "            data.to_sql(name='temp_results', con=conn, if_exists='append',index=False)\n",
    "            \n",
    "         \n",
    "        select_all = '''SELECT key, GROUP_CONCAT(value)\n",
    "                        FROM temp_results\n",
    "                        GROUP BY key\n",
    "                        ORDER BY key'''\n",
    "        \n",
    "        rows = self.cursor.execute(select_all).fetchall()\n",
    "        keys = [row[0] for row in rows]\n",
    "        documents = [row[1] for row in rows]\n",
    "        \n",
    "        threads = list()\n",
    "        for index, key, documents in zip(range(len(rows)), keys, documents):\n",
    "            x = threading.Thread(target=self.auxilary_writing_function, args=(reduce_function, index, 'mapreducefinal/part-{index}-final.csv', key, documents))\n",
    "            threads.append(x)\n",
    "            x.start()\n",
    "\n",
    "        for index, thread in enumerate(threads):\n",
    "            thread.join()\n",
    "        \n",
    "        return 'MapReduce Completed' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement all of the class here\n",
    "\n",
    "class MapReduceEngine():\n",
    "    def __init__(self, connection, cursor):\n",
    "        self.conn = connection\n",
    "        self.cursor = cursor\n",
    "    \n",
    "    def auxilary_writing_function(self, function, index, title, *args):\n",
    "        return_val = function(*args)\n",
    "        with open(eval(f'f\"{title}\"'), 'w') as f:\n",
    "            f.write('key,value\\n')\n",
    "            for val in return_val:\n",
    "                f.write(f'{val}\\n')\n",
    "    \n",
    "    def execute(self, input_data, map_function, reduce_function, params):\n",
    "\n",
    "        threads = list()\n",
    "        for index, key in zip(range(len(input_data)), input_data):\n",
    "            x = threading.Thread(target=self.auxilary_writing_function, args=(map_function, index, f'mapreducetemp/part-tmp-{index}.csv', key, params[key]))\n",
    "            threads.append(x)\n",
    "            x.start()\n",
    "\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "        \n",
    "        for index in range(len(input_data)):\n",
    "            data = pd.read_csv(f'mapreducetemp/part-tmp-{index}.csv')\n",
    "            data.to_sql(name='temp_results', con=conn, if_exists='append',index=False)\n",
    "            \n",
    "         \n",
    "        select_all = '''SELECT key, GROUP_CONCAT(value)\n",
    "                        FROM temp_results\n",
    "                        GROUP BY key\n",
    "                        ORDER BY key'''\n",
    "        \n",
    "        rows = self.cursor.execute(select_all).fetchall()\n",
    "        keys = [row[0] for row in rows]\n",
    "        documents = [row[1] for row in rows]\n",
    "        \n",
    "        threads = list()\n",
    "        for index, key, documents in zip(range(len(rows)), keys, documents):\n",
    "            x = threading.Thread(target=self.auxilary_writing_function, args=(reduce_function, index, f'mapreducefinal/part-{index}-final.csv', key, documents))\n",
    "            threads.append(x)\n",
    "            x.start()\n",
    "\n",
    "        for index, thread in enumerate(threads):\n",
    "            thread.join()\n",
    "        \n",
    "        return 'MapReduce Completed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# Question 3\n",
    "## Implement the MapReduce Inverted index of the JSON documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function `inverted_map(document_name)` which reads the CSV document from the local disc and return a list that contains entries of the form (key_value, document name).\n",
    "\n",
    "For example, if myCSV4.csv document has values like:  \n",
    "`{‘firstname’:’John’,‘secondname’:’Rambo’,‘city’:’Palo Alto’}`\n",
    "\n",
    "Then `inverted_map(‘myCSV4.csv’)` function will return a list:  \n",
    "`[(‘firstname_John’,’ myCSV4.csv’),(‘secondname_Rambo’,’ myCSV4.csv’), (‘city_Palo Alto’,’ myCSV4.csv’)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_map(document_name, column_index):\n",
    "    df = pd.read_csv(document_name)\n",
    "    file_abs_path = os.path.abspath(document_name)\n",
    "    return [f'{key}, {file_abs_path}' for key in df.iloc[:, column_index - 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a reduce function `inverted_reduce(value, documents)`, where the field “documents” contains a list of all CSV documents per given value.   \n",
    "This list might have duplicates.   \n",
    "Reduce function will return new list without duplicates.\n",
    "\n",
    "For example,  \n",
    "calling the function `inverted_reduce(‘firstname_Albert’,’myCSV2.csv, myCSV5.csv,myCSV2.csv’)`   \n",
    "will return a list `[‘firstname_Albert’,’myCSV2.csv, myCSV5.csv,myCSV2.csv’]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_reduce(key, documents):\n",
    "    documents = documents.split(',')\n",
    "    documents = set(documents)\n",
    "    new_documents = ','.join(documents)\n",
    "    return [f'{key}, {new_documents}']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "# Question 4\n",
    "## Testing Your MapReduce\n",
    "\n",
    "**Create Python list** `input_data` : `[‘myCSV1.csv’,.. ,‘myCSV20.csv’]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [f'csvs/myCSV[{i}].csv' for i in range(20)]\n",
    "params = {val: 1 for val in input_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submit MapReduce as follows:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MapReduce Completed 0.07785329100000027\n"
     ]
    }
   ],
   "source": [
    "mapreduce = MapReduceEngine(conn, cursor)\n",
    "t_start = timeit.default_timer()\n",
    "status = mapreduce.execute(input_data, inverted_map, inverted_reduce, params)\n",
    "t_stop = timeit.default_timer()\n",
    "print(status, t_stop - t_start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MapReduce Completed 0.05071649999999295\n"
     ]
    }
   ],
   "source": [
    "mapreduce = MapReduceEngine_smallFiles(conn, cursor, block_size=1)\n",
    "t_start = timeit.default_timer()\n",
    "status = mapreduce.execute(input_data, inverted_map, inverted_reduce, params)\n",
    "t_stop = timeit.default_timer()\n",
    "print(status, t_stop - t_start) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that `MapReduce Completed` should be printed and `mapreducefinal` folder should contain the result files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use python to delete all temporary data from mapreducetemp folder and delete SQLite database:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    os.remove(f'mapreducetemp/part-tmp-{i}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# Question 5\n",
    "# Final Thoughts\n",
    "\n",
    "The phase where `MapReduceEngine` reads all temporary files generated by maps and sort them to provide each reducer a specific key is called the **shuffle step**.\n",
    "\n",
    "Please explain **clearly** what would be the main problem of MapReduce when processing Big Data, if there is no shuffle step at all, meaning reducers will directly read responses from the mappers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            If you say \"I dont know\" you will get 2 points :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I understand it, lacking a shuffle means that there is no guarantee that the reduce will be calculated on a machine which is close to its data -- the result of the map function -- therefore data would have to be transferred over the net, which is a problem the platform (say, hadoop) is trying to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "Good Luck :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
